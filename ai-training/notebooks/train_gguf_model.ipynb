{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D&D AI Model Training with Unsloth and Gemma3N\n",
    "\n",
    "This notebook fine-tunes a Gemma3N model specifically for D&D gameplay using Unsloth for efficient training and exports to GGUF format for local deployment with Cactus AI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Base Model**: Gemma3N (2B/4B variants)\n",
    "- **Training Framework**: Unsloth for 2x faster training with 70% less VRAM\n",
    "- **Output Format**: GGUF for Ollama/llama.cpp compatibility\n",
    "- **Target Application**: Cactus AI integration for D&D gameplay\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.9-3.11\n",
    "- 16GB+ RAM (32GB recommended)\n",
    "- GPU with 8GB+ VRAM (16GB recommended) or Apple Silicon Mac\n",
    "- 50GB+ free disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'homebrew (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import torch\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "# System information\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS Available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")\n",
    "print(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Install Unsloth and required packages. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install unsloth[colab-new] --quiet\n",
    "\n",
    "# Install additional required packages\n",
    "!pip install datasets transformers accelerate bitsandbytes --quiet\n",
    "\n",
    "# For GGUF export\n",
    "!pip install llama-cpp-python --quiet\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import markdown\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"unsloth/gemma-3-2b-it\",  # Can also use \"unsloth/gemma-3-4b-it\"\n",
    "    \"max_seq_length\": 2048,  # Max context length\n",
    "    \"load_in_4bit\": True,    # 4-bit quantization for memory efficiency\n",
    "    \"dtype\": None,           # Auto-detect best dtype\n",
    "    \n",
    "    # LoRA settings\n",
    "    \"r\": 16,                 # LoRA rank\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,       # No dropout for better convergence\n",
    "    \"bias\": \"none\",\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",\n",
    "    \"random_state\": 42,\n",
    "    \"use_rslora\": False,     # Set to True for large datasets\n",
    "    \"loftq_config\": None,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"max_steps\": 60,         # Adjust based on dataset size\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\": torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Output settings\n",
    "    \"output_dir\": \"./trained_models\",\n",
    "    \"model_save_name\": \"dnd_gemma3_2b\",\n",
    "    \"gguf_quantization\": \"q4_k_m\",\n",
    "    \n",
    "    # D&D specific settings\n",
    "    \"system_message\": \"You are a Dungeon Master assistant for D&D 5e. You help with gameplay, rules, and story generation. Use tool calls when needed for game mechanics.\",\n",
    "    \"tool_format\": \"[{tool_name}: {arguments}]\",\n",
    "    \"supported_tools\": [\"roll\", \"health\", \"inventory\", \"spellcast\", \"check\", \"save\"]\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dtype=CONFIG[\"dtype\"],\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Configure LoRA for fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG[\"r\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=CONFIG[\"bias\"],\n",
    "    use_gradient_checkpointing=CONFIG[\"use_gradient_checkpointing\"],\n",
    "    random_state=CONFIG[\"random_state\"],\n",
    "    use_rslora=CONFIG[\"use_rslora\"],\n",
    "    loftq_config=CONFIG[\"loftq_config\"],\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured successfully!\")\n",
    "print(f\"Trainable parameters: {model.get_nb_trainable_parameters()[0]:,}\")\n",
    "print(f\"Training efficiency: {model.get_nb_trainable_parameters()[1]:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homebrew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
